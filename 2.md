# LLM組み込みシステムの障害と新しいオブザーバビリティ

## 1. 現代のシステム課題：LLM導入で障害が止まらない

### 従来システムの前提が壊れた根本原因

**従来システムの前提**

- インプット → 決定論的な処理 → アウトプット（固定）
- 入出力が決まっていた
- 処理の流れがコードで追跡可能
- 障害の原因特定が比較的容易

**LLM導入後の現実**

- インプット → LLM（ブラックボックス） → 揺らぐアウトプット
- 同じインプットでも異なるアウトプットが発生
- 中身がブラックボックス化
- ログはあると言われても説明できない
- **「LLMの中身がわかりません」** ← 最大の問題

-----

## 2. なぜ課題が生じるのか

### 観測の対象が従来通りでは意味をなさない

**従来のオブザーバビリティで見ていた指標**

```
CPU使用率  → 異常なし
メモリ使用量 → 問題ない
ディスクI/O → 正常
ネットワーク → 遅延なし

↓ なのに、なぜか回答品質が悪い？
```

**根本的な問題**

- インフラレベルのメトリクスは健全でもLLMの出力品質は低下する
- LLMの「揺らぎ」は従来的なメトリクスには反映されない
- 構造がわかる観測対象（コード）から、構造がわからない観測対象（モデル）へ転換

-----

## 3. AI時代のオブザーバビリティ設計

### 見るべき対象の根本的な転換

|従来のシステム  |LLM組み込みシステム    |
|---------|---------------|
|**何を見るか**|CPUメモリ、レスポンスタイム|
|**原因追跡** |コードの流れをトレース    |
|**エラー判定**|例外、ステータスコード    |

### LLM時代のキー観測指標

**5つの主要シグナル**

#### 1. **Prompt（プロンプト）**

- インプットされたプロンプトの内容
- プロンプトエンジニアリングの影響を可視化
- 同じプロンプトでも結果が異なる場合の比較対象

#### 2. **Completion（完成度・回答）**

- LLMが生成した実際のテキスト
- 品質が期待値に達しているか
- 回答が不適切な場合の証跡

#### 3. **Tokens（トークン数）**

- **最大の見落とし指標**
- プロンプト変更でトークンが爆増する
- 実行コストに直結（トークンベースの課金）
- **サイレントキラー** ← 気づきにくい被害

#### 4. **Latency（レイテンシー）**

- **揺らぎ があるか（重要）**
- レイテンシーがコンスタントか変動するか
- 遅いパーツはどこか？
  - 推論自体か
  - データ収集か
  - 前処理か
- 原因特定の手がかり

#### 5. **Errors（エラー）**

- LLM特有のエラー
  - ポリシー違反（出力制限）
  - トークン数超過
  - API呼び出し失敗
  - モデル側のエラー
- 従来のHTTPステータスコードでは把握不可

-----

## 4. LLMオブザーバビリティの実装設計

### 問題：エラー率だけでは何もわからない

```
エラー率：2% ← 見えたとしても...

「なぜ、その2%は不正な回答を出したのか？」
「コンテキストの中身は何だったのか？」
「プロンプトは何を要求していたのか？」

↓ これらは従来のメトリクスでは追跡不可能
```

### 解決策：リクエストレベルでの詳細トレース

**観測すべき単位の転換**

|レベル|従来の観測        |LLM時代の観測             |
|---|-------------|---------------------|
|マクロ|エラー率、平均レイテンシー|トークン数の分布、出力品質の分散     |
|ミクロ|ステータスコード     |**個別のリクエストコンテキストの中身**|

**実装：リクエストのコンテキスト記録**

```yaml
リクエストごとに記録する情報:
  - Prompt: ユーザーの質問内容
  - Model: 使用したLLMモデル
  - Temperature: 生成パラメータ
  - Tokens_used: 実際に使用したトークン数
  - Latency_ms: 応答時間
  - Completion: LLMが返した回答
  - Quality_score: 期待値との照合結果
  - Timestamp: いつ実行されたか
```

-----

## 5. Datadog等での設計ポイント

### 従来の「Logs, Traces, Metrics」の暗黙の前提の破綻

**従来の前提**

```
「処理の中身がコードである」
↓
Logsでステップを追跡
Tracesで依存関係を可視化
Metricsでシステム全体を監視

これで十分だった
```

**LLMが入ると**

```
「処理の中身がモデルである」
↓
Logsに「モデルが何を出力したか」を記録しても
「なぜそれが出力されたのか」は不明

Tracesで依存関係を見ても
「モデル内部で何が起きたか」は見えない

Metricsで上位レイテンシーを検出しても
「どのプロンプト、どのモデル」が原因かは追跡困難
```

### ギャップを埋める「LLMオブザーバビリティ」層

**3層構造の導入**

```
┌─────────────────────────────────────┐
│ 従来のオブザーバビリティ             │
│ (Logs, Traces, Metrics)             │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ LLMオブザーバビリティ（新層）       │
│ - Prompt/Completion記録              │
│ - トークン数追跡                     │
│ - 出力品質評価                       │
│ - コンテキスト全体の可視化           │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ ビジネス影響の可視化                 │
│ - コスト (トークン数 × 課金率)      │
│ - 品質スコア                         │
│ - ユーザー満足度                     │
└─────────────────────────────────────┘
```

-----

## 6. トークン数：サイレントキラーへの対策

### なぜトークン数が見落とされるのか

**見落とされやすい理由**

- インフラ的には「正常に動作」している
- ネットワークやCPUの目立った異常がない
- しかし、使うほどお金が掛かる
- 気づいた時には運用ダメージが出ている

**実例：プロンプト変更の落とし穴**

```
変更前プロンプト：「質問に答えて」
→ トークン数：平均 150トークン

変更後プロンプト：「詳しく背景を説明した上で、質問に答えて」
→ トークン数：平均 500トークン（3倍以上！）

結果：月額コストが同時に3倍以上に
```

### 対策：トークン数の監視体制

**実装アプローチ**

1. **プロンプト変更時のトークン計測**
   
   ```
   プロンプト変更 → 本番デプロイ前に
   サンプルリクエスト × 100件で予測トークン数を計算
   → コスト影響を事前評価
   ```
1. **リアルタイムアラート設定**
   
   ```
   アラート1: 1リクエスト当たりのトークン数が
              通常の150%を超えた
   
   アラート2: 日次トークン数が昨日の120%を超えた
   
   アラート3: 特定プロンプントークン数が
              直近7日平均の200%を超えた
   ```
1. **トークン数の内訳可視化**
   
   ```
   入力トークン vs 出力トークン
   
   例：入力800 + 出力200 = 1000トークン
   
   → 入力が多い場合：
      「プロンプトに含めるコンテキストが肥大化」
   
   → 出力が多い場合：
      「LLMが冗長な回答を生成」
   ```

-----

## 7. 再現できない問題への対応：トレース駆動

### 従来のデバッグ手法の限界

**従来の方法：再現実験**

```
「問題が起きました」
↓
「同じ条件で再現させる」
↓
「ログを見ながら追跡」
↓
「原因特定」
```

**LLM組み込みシステムでは不可能**

```
同じプロンプト、同じパラメータでも
LLMの出力が異なる可能性がある

→ 再現実験そのものが成り立たない
```

### 解決策：実際のトレースの記録と分析

**実装方針**

1. **本番環境での全リクエスト記録**
   
   ```
   - 全ユーザーリクエストのプロンプト
   - 全LLM応答
   - 全パラメータ
   - タイムスタンプ
   ```
1. **問題が発生したリクエストの特定**
   
   ```
   問題報告 → ユーザーID、時刻から
            → 該当リクエストのコンテキスト全体を取得
            → なぜ不正な回答が出たのかを分析
   ```
1. **類似パターンの事前検出**
   
   ```
   その1つのトレースから
   「同じプロンプトパターンの他のリクエスト」を抽出
   
   → 被影響範囲の推定
   → 同じバグが他に存在しないか確認
   ```

**メリット**

- 再現実験不要：実際のデータが証跡
- 迅速な対応：すぐに該当リクエストにアクセス可能
- パターン検出：1件の障害から全体への影響を推測

-----

## 8. 品質観測：振る舞いそのものを見る

### エラー率では見えない品質問題

```
"エラー" として検出される
・APIエラー（トークン超過など）
・ポリシー違反
・接続エラー

"エラー" として検出されない（危険）
・不正確な回答
・ハルシネーション（作り話）
・不適切な語調
・期待値との乖離
```

### 品質指標の設計

**振る舞い観測の3層**

|層       |観測対象      |測定方法                        |
|--------|----------|----------------------------|
|**レベル1**|出力の揺らぎの度合い|同じプロンプトに対する出力のばらつき度         |
|**レベル2**|出力の妥当性    |期待値（Reference Answer）との比較スコア|
|**レベル3**|ユーザー満足度   |ユーザーがThumbsUp/Downを押すフィードバック|

**実装例：品質スコア**

```yaml
Quality_Score:
  - Relevance: プロンプトに対する関連性 (0-100)
  - Accuracy: 正確性の検証 (0-100)
  - Coherence: 一貫性・読みやすさ (0-100)
  - Safety: ポリシー準拠度 (0-100)
  - Average: 総合スコア
```

-----

## 9. コスト観測：運用ダメージの可視化

### トークンコスト構造の把握

**単純な計算**

```
月間トークン数 × トークン単価 = 月額コスト

例：
100万トークン × $0.001/1000トークン = $1/月
↓ (プロンプト変更で3倍に)
300万トークン × $0.001/1000トークン = $3/月
```

### コスト観測の詳細設計

**観測単位別のコスト追跡**

```
組織全体のコスト
  ├─ 部門別コスト
  │   ├─ 営業部: $500/月
  │   ├─ サポート部: $2,000/月
  │   └─ 企画部: $300/月
  │
  ├─ 機能別コスト
  │   ├─ チャットボット: $1,800/月
  │   ├─ ドキュメント生成: $700/月
  │   └─ コード補完: $300/月
  │
  └─ ユーザー別コスト
      ├─ パワーユーザーA: $150/月
      ├─ 通常ユーザーB: $20/月
      └─ ライトユーザーC: $5/月
```

**アラート設定例**

- 前月比コストが150%を超えた
- 特定ユーザーが1日のコスト上限（$10）を超えた
- 1リクエスト当たりの平均コストが2倍に跳ね上がった

-----

## 10. まとめ：LLMオブザーバビリティの実装ロードマップ

### Phase 1：基礎的な観測（Week 1-2）

- ✅ Prompt/Completion の全記録開始
- ✅ トークン数の日次集計
- ✅ レイテンシー分布の把握

### Phase 2：品質観測（Week 3-4）

- ✅ 出力品質スコアの自動計測
- ✅ エラータイプの分類と追跡
- ✅ ユーザーフィードバック機構の実装

### Phase 3：コスト管理（Month 2）

- ✅ トークンコストの部門別・機能別追跡
- ✅ アラート設定と自動通知
- ✅ コスト最適化レポートの自動生成

### Phase 4：高度な分析（Month 3+）

- ✅ プロンプト変更の事前コスト影響評価
- ✅ 異常パターンの自動検出
- ✅ 根本原因分析（RCA）の自動化

-----

## 最後に：LLM時代の観測哲学

**従来**

```
「システムが正常に動いている」
= CPU、メモリ、ネットワークが健全
```

**LLM時代**

```
「システムが正常に動いている」
= Prompt、Completion、Tokenが期待値の範囲内
+ コスト効率が保たれている
+ 出力品質が維持されている
```

**勝負の分かれ目**：リクエストコンテキストの「全体」を1つの単位として観測できるシステムを構築した組織が、LLMの課題に迅速に対応できる。